{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Cleaning\n",
    "\n",
    "First, we import the necessary libraries for data manipulation, numerical operations, and preprocessing. Pandas is used for data manipulation, NumPy for numerical operations, and train_test_split from scikit-learn for splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Py_Files\\fake news\\torchenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our dataset from a CSV file named 'dataset-merged.csv' and inspect the column names to understand the structure of our data. This helps us identify any unnecessary columns that need to be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset-merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sr', 'text', 'label', 'wcount'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'sr' is identified as redundant and is removed from the dataset. We then check for missing values in the dataset to ensure data integrity. If there are any missing values, they could potentially skew our analysis and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label', 'wcount'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('sr', axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "label     0\n",
       "wcount    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the text data, we define a preprocessing function to remove digits and extra spaces. This helps in normalizing the text data before tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "We split the dataset into training and test sets using an 80-20 split. This ensures that our models can be trained and tested on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with BERT\n",
    "\n",
    "We utilize the BERT tokenizer to tokenize our text data. BERT is a powerful pre-trained language model that helps in capturing the context of words in a sentence. We set a maximum sequence length of 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Py_Files\\fake news\\torchenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAM\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "e:\\Py_Files\\fake news\\torchenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Useing BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "max_len = 100  # Maximum length of the sequences\n",
    "X_train_tokens = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "X_test_tokens = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting labels to Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "\n",
    "To handle batching of the tokenized data, we create a custom dataset class that extends torch.utils.data.Dataset. This class returns the tokenized inputs and corresponding labels. We then create DataLoaders for training and test datasets to enable easy batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset and dataloader to handle batching\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = NewsDataset(X_train_tokens, y_train)\n",
    "test_dataset = NewsDataset(X_test_tokens, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Definition\n",
    "\n",
    "We define an LSTM classifier for our text classification task. LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) capable of learning long-term dependencies, making them suitable for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of LSTM Layers\n",
    "- Embedding Layer: Converts input text tokens into dense vectors of a fixed size (embedding_dim).\n",
    "- LSTM Layer: The core of the model, which processes sequences of data. It has parameters such as hidden_dim (size of the hidden state).\n",
    "- n_layers (number of LSTM layers stacked), bidirectional (if True, the LSTM will be bidirectional), and dropout (dropout probability).\n",
    "- Fully Connected Layer: Maps the hidden state output of the LSTM to the desired output dimension (number of classes).\n",
    "- Dropout Layer: Regularization technique to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Model Initialization\n",
    "\n",
    "Hyperparameter tuning is a crucial step in building an effective machine learning model, as it involves selecting the optimal set of hyperparameters that define the model's structure and learning process. In this project, we carefully set the hyperparameters for our LSTM model to achieve a balance between model complexity and performance. The embedding dimension (embedding_dim) is set to 128, which determines the size of the vector space in which words will be embedded. The hidden dimension (hidden_dim) is set to 256, defining the size of the hidden states in the LSTM layers, which allows the model to capture more complex patterns in the data. We use 8 LSTM layers (n_layers), enabling the model to learn hierarchical representations of the text data. The bidirectional flag (bidirectional) is set to True, allowing the LSTM to capture dependencies in both forward and backward directions, thus improving the model's context understanding. The dropout rate (dropout) is set to 0.3, which helps in regularizing the model by preventing overfitting. The vocabulary size (vocab_size) is derived from the BERT tokenizer, ensuring compatibility with pre-trained BERT embeddings. These hyperparameters are chosen based on prior experiments and domain knowledge, and further fine-tuning can be performed using grid search or random search to find the optimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 8\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# initailaizing the model\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, vocab_size, output_dim, n_layers, bidirectional, dropout)\n",
    "\n",
    "# Move the model to CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "The choice of the loss function and optimizer significantly impacts the model's training process and final performance. For this text classification task, we use the Cross-Entropy Loss (nn.CrossEntropyLoss()), which is well-suited for multi-class classification problems. This loss function measures the difference between the predicted probability distribution and the true distribution of the classes, penalizing incorrect predictions more severely. It is particularly effective in dealing with imbalanced datasets where some classes might be more frequent than others.\n",
    "\n",
    "To optimize the model parameters, we employ the Adam optimizer (optim.Adam). Adam (short for Adaptive Moment Estimation) is an advanced optimization algorithm that combines the advantages of two other popular methods: AdaGrad and RMSProp. It adapts the learning rate for each parameter individually, using estimates of first and second moments of the gradients. This makes Adam particularly well-suited for training deep neural networks, as it can handle sparse gradients and noisy data more efficiently. We set the learning rate to a small value of 0.0001, which helps in achieving a more stable and gradual convergence, reducing the risk of overshooting the optimal parameter values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up loss function and Adam optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "We define functions to train and evaluate the model. The train_model function performs one epoch of training, and the evaluate_model function evaluates the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "    return total_loss / len(test_loader), correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We train the model for a specified number of epochs and print the validation loss and accuracy at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAM\\AppData\\Local\\Temp\\ipykernel_10080\\2606137705.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SAM\\AppData\\Local\\Temp\\ipykernel_10080\\2606137705.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.5538217243221071, Val Acc: 0.7191240875912409\n",
      "Epoch 2, Val Loss: 0.4908814444034188, Val Acc: 0.7693430656934307\n",
      "Epoch 3, Val Loss: 0.5684072830610805, Val Acc: 0.7170802919708029\n",
      "Epoch 4, Val Loss: 0.419455757709565, Val Acc: 0.8186861313868613\n",
      "Epoch 5, Val Loss: 0.4856743061984027, Val Acc: 0.7944525547445256\n",
      "Epoch 6, Val Loss: 0.41693154184354675, Val Acc: 0.8294890510948905\n",
      "Epoch 7, Val Loss: 0.3838138275400356, Val Acc: 0.8426277372262774\n",
      "Epoch 8, Val Loss: 0.4031199230640023, Val Acc: 0.8198540145985401\n",
      "Epoch 9, Val Loss: 0.3804402712870527, Val Acc: 0.8446715328467154\n",
      "Epoch 10, Val Loss: 0.38689802380071747, Val Acc: 0.8458394160583942\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}, Val Loss: {val_loss}, Val Acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation\n",
    "\n",
    "Finally, we evaluate the trained model on the test set to obtain the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAM\\AppData\\Local\\Temp\\ipykernel_10080\\2606137705.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SAM\\AppData\\Local\\Temp\\ipykernel_10080\\2606137705.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.38689802380071747, Test Accuracy: 0.8458394160583942\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "The model achieves a test accuracy of approximately 84.58%, demonstrating its effectiveness in classifying the news articles. The training and evaluation functions help in monitoring the performance of the model throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can incorporate Learning Rate Scheduler and Early Stopping in our model trainig. See the results on - https://github.com/Spinal-Tap369/fake-news/blob/main/hindi_fake_news_lstm_lrs_es.ipynb "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
